What is the name of the new network architecture proposed in the paper?
Answer: The paper introduces the Transformer architecture. 

Which sequence-transduction models does the Transformer aim to replace?
Answer: It is explicitly designed to replace encoderâ€“decoder models that rely on recurrent (RNN/LSTM/GRU) or convolutional layers. The Transformer is the first sequence-to-sequence architecture based entirely on attention, rather than on recurrent or convolutional modules. 

What are the three types of sub-layers in the Transformerâ€™s decoder stack?
Answer: Each decoder layer contains:

A masked multi-head self-attention sub-layer

A multi-head attention over the encoderâ€™s outputs sub-layer (often called â€œencoderâ€“decoder attentionâ€)

A position-wise (fully connected) feed-forward sub-layer 

How many identical layers does the Transformerâ€™s encoder have, and what is the dimensionality 
ğ‘‘
model
d 
model
â€‹
  used throughout?
Answer: The encoder is a stack of 
ğ‘
=
6
N=6 identical layers, and the model dimensionality 
ğ‘‘
model
d 
model
â€‹
  is 512. 