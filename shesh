What is the name of the new network architecture proposed in the paper?
Answer: The paper introduces the Transformer architecture. 

Which sequence-transduction models does the Transformer aim to replace?
Answer: It is explicitly designed to replace encoder–decoder models that rely on recurrent (RNN/LSTM/GRU) or convolutional layers. The Transformer is the first sequence-to-sequence architecture based entirely on attention, rather than on recurrent or convolutional modules. 

What are the three types of sub-layers in the Transformer’s decoder stack?
Answer: Each decoder layer contains:

A masked multi-head self-attention sub-layer

A multi-head attention over the encoder’s outputs sub-layer (often called “encoder–decoder attention”)

A position-wise (fully connected) feed-forward sub-layer 

How many identical layers does the Transformer’s encoder have, and what is the dimensionality 
𝑑
model
d 
model
​
  used throughout?
Answer: The encoder is a stack of 
𝑁
=
6
N=6 identical layers, and the model dimensionality 
𝑑
model
d 
model
​
  is 512. 