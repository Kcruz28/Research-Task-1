What does BERT stand for?
Bidirectional Encoder Representations from Transformers.

Which company developed BERT?
Google AI Language.

What is the main goal of BERT?
To pre-train deep bidirectional representations from unlabeled text for improved language understanding.

What type of model architecture does BERT use?
A multi-layer bidirectional Transformer encoder.

What is the purpose of the [CLS] token in BERT?
It is a special classification token used as the aggregate sequence representation for classification tasks.

What is the purpose of the [SEP] token in BERT?
It is a special separator token used to separate sentences or segments in the input sequence.

What does the Masked Language Model (MLM) task involve?
Randomly masking some input tokens and training the model to predict the original tokens based on context.

What is the Next Sentence Prediction (NSP) task?
A pre-training task where the model predicts whether two sentences are consecutive in the original text.

Which two large text corpora were used to pre-train BERT?
BooksCorpus (800M words) and English Wikipedia (2,500M words).

How many parameters does BERTBASE have?
110 million parameters.

How does BERT differ from previous unidirectional language models?
BERT is deeply bidirectional, considering both left and right context for each token, unlike previous models that were unidirectional (left-to-right or right-to-left).

Why is bidirectionality important for BERT’s performance?
It allows the model to capture richer contextual information and dependencies, improving understanding and performance on various NLP tasks.

How does BERT handle input sequences containing sentence pairs?
It packs both sentences into a single sequence, separates them with a [SEP] token, and uses segment embeddings to distinguish them.

What masking strategy does BERT use during pre-training for MLM?
For each selected token: 80% are replaced with [MASK], 10% with a random token, and 10% remain unchanged.

Why does BERT sometimes replace masked tokens with random tokens or keep them unchanged?
To reduce the mismatch between pre-training and fine-tuning, since [MASK] tokens do not appear during fine-tuning.

How does BERT’s pre-training procedure help with downstream tasks?
It learns general language representations that can be fine-tuned for specific tasks, leading to improved performance with minimal task-specific changes.

What is the difference between feature-based and fine-tuning approaches in using pre-trained models?
Feature-based uses pre-trained representations as additional features in task-specific models, while fine-tuning updates all parameters for the downstream task.

How does BERT perform on the GLUE benchmark compared to previous models?
BERT achieves state-of-the-art results, significantly outperforming previous models on all GLUE tasks.

What are the main advantages of BERT for question answering tasks?
Its bidirectional context and unified architecture enable superior understanding and extraction of answers from passages, resulting in top performance on benchmarks like SQuAD.

How does BERT’s architecture allow it to be adapted to different NLP tasks with minimal changes?
The same pre-trained model can be fine-tuned for various tasks by adding simple output layers, without major modifications to the core architecture